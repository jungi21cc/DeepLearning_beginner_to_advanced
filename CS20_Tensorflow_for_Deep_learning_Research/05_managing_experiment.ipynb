{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 lecture\n",
    "# variable_sharing_and_managing_experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# structure its tensorflow model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# phase1 assemble graph\n",
    "1. import data with tf.data or placeholders\n",
    "2. define the weights\n",
    "3. define the inference model\n",
    "4. define loss function\n",
    "5. define optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model as class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jk/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModel:\n",
    "    '''build the graph for word2vec model'''\n",
    "    def __init__(self, params):\n",
    "        pass\n",
    "    \n",
    "    def _import_data(self):\n",
    "        '''step1 : import data'''\n",
    "        pass\n",
    "    \n",
    "    def _create_embedding(self):\n",
    "        '''step2 : define weights. In word2vec, its actaully the weights that we care awbout'''\n",
    "        pass\n",
    "\n",
    "    def _create_loss(self):\n",
    "        '''step3, 4 : define the inference + loss function'''\n",
    "        pass\n",
    "    \n",
    "    def _create_optimizer(self):\n",
    "        '''setp5 : define optimizer'''\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# name scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# variable scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.get_variable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.variable_scope()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reuse varialbe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fully_connected(x, output_dim, scope):\n",
    "    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE) as scope:\n",
    "        w = tf.get_variable('weights', [x.shape[1], output_dim], initializer=tf.random_normal_initializer())\n",
    "        b = tf.get_variable('biases', [output_dim], initializer=tf.constant_initializer(0.0))\n",
    "        \n",
    "def two_hidden_layers(x):\n",
    "    h1 = fully_connected(x, 50, 'h1')\n",
    "    h2 = fully_connected(h1, 10, 'h2')\n",
    "    \n",
    "# with tf.variable_scope('two_layers') as scope:\n",
    "#     logits1 = two_hidden_layers(x1)\n",
    "#     logits2 = two_hidden_layers(x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.train.Saver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save sessions, not graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define model\n",
    "model = SkipGramModel(params)\n",
    "\n",
    "#create a saver object\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for step in range(training_steps):\n",
    "        sess.run([optimizer])\n",
    "        \n",
    "        #save model every 1000steps\n",
    "        if (step + 1) % 1000 == 0:\n",
    "            saver.save(sess, \n",
    "                       'checkpoint_directory/model_name',\n",
    "                       global_steop=step\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# global step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(lr).minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.train.Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = tf.Variable(..., name='v1')\n",
    "v2 = tf.Variable(..., name='v2')\n",
    "\n",
    "#one way\n",
    "saver = tf.train.Saver({'v1':v1, 'v2':v2})\n",
    "#another way\n",
    "saver = tf.train.Saver([v1, v2])\n",
    "#another way\n",
    "saver = tf.train.Saver({v.op.name: v for v in [v1, v2]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# restore variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver.restore(sess, 'checkpoints/name_of_the_checkpoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# restore the latest checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if there is checkpoint\n",
    "ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/checkpoint'))\n",
    "\n",
    "#check if there is a valid checkpoint path\n",
    "if ckpt and ckpt,model_checkpoint_path:\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step1 : create summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('summaries'):\n",
    "    tf.summary.scalar('loss', self.loss)\n",
    "    tf.summary.scalar('accuracy', self.accuracy)\n",
    "    tf.summary.histogram('histogram loss', self.loss)\n",
    "    summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step2 : run them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_batch, _, summary = sess.run([loss, optimizer, summary_op])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step3 : write summaries to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_summary(summary, global_step=step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# putting it together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.summary.scalar('loss', self.loss)\n",
    "tf.summary.histogram('histogram loss', self.loss)\n",
    "summary_op = tf.summary.merge_all()\n",
    "\n",
    "saver = tf.train.Saver() # defaults to saving all variables\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variable_initializer())\n",
    "    ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/checkpoint'))\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        \n",
    "    writer = tf.summary.FileWriter('./graphs', sess.graph)\n",
    "    for index in range(1000):\n",
    "        ...\n",
    "        loss_batch, _, summary = sess.run([loss, optimizer, summary_op])\n",
    "        writer.add_summary(summary, global_step=index)\n",
    "        \n",
    "        if (index + 1) % 1000 == 0:\n",
    "            saver.save(sess, 'checkpoints/skip-gram', index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# global level seed & operation level seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.gradients(y, [xs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(2.0)\n",
    "y = 2.0 * (x ** 3)\n",
    "z = 3.0 + y ** 2\n",
    "\n",
    "grad_z = tf.gradients(z, [x, y])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(x.initializer)\n",
    "    print(sess.run(grad_z)) # >> [768.0, 32.0]\n",
    "    \n",
    "# 768 is the gradient of z with respect to x, 32 with respect to y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gradient computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.gradients(ys, xs, grad_ys=None,....)\n",
    "tf.stop_gradient(input, name=None,....)\n",
    "#prevents the contribution of its inputs to be taken into account\n",
    "tf.clip_by_value(t, clip_value_min, clip_value_max, name=None)\n",
    "tf.clip_by_norm(t, clip_norm, axes=None, name=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
